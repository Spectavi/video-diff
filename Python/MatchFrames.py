from __future__ import print_function

import os
import sys

import cv2
import numpy as np

import Clustering
import common
import common_cv
import config
import ECC  # TODO: This file does not exist?
import LK
import Misc
import SimAnneal


FLANN_INDEX_KDTREE = 1  # bug: flann enums are missing
FLANN_INDEX_LSH = 6

'''
Feature-based image matching sample.

USAGE
  find_obj.py [--feature=<sift|surf|orb>[-flann]] [ <image1> <image2> ]

  --feature  - Feature to use. Can be sift, surf of orb. Append '-flann' to feature name
                to use Flann-based matcher instead bruteforce.

  Press left mouse button on a feature point to see its matching point.
'''

"""
The original find_obj.py script was using SIFT/SURF/ORB to extract features
    from 2 images and applied the resulting homography transformation to nicely
    determine an object from image 1 to an image containing the object to
    image 2. However, the example they provided was highly... optimistic :))

We also added:
    Lucas-Kanade's image alignment/registration algorithm from lk_homography.py
    motion history and diffs
"""


detector = None
matcher = None
num_frames_q = 0
num_frames_r = 0
counter_q = -1
counter_r = -1

kp1 = None
kp2 = None
desc1 = None
desc2 = None

img_q = None
img_r = None

"""
Generated by feature_match_and_homography() called by temporal_alignment().
    Used in rest().
"""
p1 = None
p2 = None
kp_pairs = None
status = None
H = None

"""
The keypoints matched by knnMatch() BUT FILTERED out by
    filter_keypoints_matches() called by temporal_alignment().
  nonp1 used in rest().
"""
nonp1 = None
nonp2 = None

# Created by annotate_vis
vis = None
vis_orig = None


# TODO: Lots of errors in this file.


def init_feature(name):
    chunks = name.split('-')

    # TODO: choose num of features to detect config.numFeaturesToExtractPerFrame
    #  depending on the resolution of the image

    if chunks[0] == "sift":
        detector = cv2.SIFT()
        norm = cv2.NORM_L2
    elif chunks[0] == "surf":
        detector = cv2.SURF(config.numFeaturesToExtractPerFrame)
        norm = cv2.NORM_L2
    elif chunks[0] == "orb":
        detector = cv2.ORB(config.numFeaturesToExtractPerFrame)
        norm = cv2.NORM_HAMMING
    else:
        return None, None

    if "flann" in chunks:
        if norm == cv2.NORM_L2:
            flann_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        else:
            flann_params = dict(algorithm=FLANN_INDEX_LSH,
                                table_number=6,
                                key_size=12,
                                multi_probe_level=1)
        # bug : need to pass empty dict (#1329)
        matcher = cv2.FlannBasedMatcher(flann_params, {})
    else:
        matcher = cv2.BFMatcher(norm)

    return detector, matcher


def pre_main(n_frames_q, n_frames_r):
    global detector, matcher
    global num_frames_q, num_frames_r
    global kp1, kp2
    global desc1, desc2

    """
    # From http://docs.scipy.org/doc/numpy/reference/generated/numpy.set_printoptions.html
    numpy.set_printoptions(precision=None, threshold=None, edgeitems=None, 
                           linewidth=None, suppress=None, nanstr=None,
                           infstr=None, formatter=None)
        threshold : int, optional
            Total number of array elements which trigger summarization rather
            than full repr (default 1000).
        suppress : bool, optional
            Whether or not suppress printing of small floating point values
            using scientific notation (default False).
    """
    np.set_printoptions(threshold=1000000, linewidth=3000)

    num_frames_q = n_frames_q
    num_frames_r = n_frames_r

    feature_name = config.FEATURE_DETECTOR_AND_MATCHER

    common.DebugPrint("Alex: feature_name = %s" % feature_name)

    detector, matcher = init_feature(feature_name)
    if detector is not None:
        print("using %s" % feature_name)
    else:
        print("unknown feature: %s" % feature_name)
        sys.exit(1)

    kp1 = [None] * num_frames_q
    kp2 = [None] * num_frames_r
    desc1 = [None] * num_frames_q
    desc2 = [None] * num_frames_r


# SPATIAL ALIGNMENT

"""
spatial_alignment() computes the homography, aligns and computes the warped
    image and returns vis with the input frame and the warped reference frame.
    Further processing of these frames is performed in annotate_vis().

Note that status is obtained from computing the homography with
            cv2.findHomography() (from feature_match_and_homography(),
                                called by temporal_alignment()).

We annotate the visual (the pair of matched frames) with the detected
    features/keypoints and the meaningful clusters of non-matched features
    of the input frame.

Normally H is the homography obtained from the temporal alignment.
"""


def spatial_alignment(win, input_frame, ref_frame, kp_pairs, status=None,
    h=None):
    h_q, w_q = input_frame.shape[:2]
    h_r, w_r = ref_frame.shape[:2]

    vis = None

    # Alex: if it's an inlier we draw a line, etc

    """
    Alex: we convert images to gray, otherwise we get exception like:
      Traceback (most recent call last):
        File "ReadAVI.py", line 321, in <module>
          Main()
        File "ReadAVI.py", line 258, in Main
          res = find_obj.ComputeFeatures2(ref_frame, counter_r)
        File "Z:\1PhD\UPB_RO\CV_Video_Based_Detection\1\find_obj.py", line 387, in ComputeFeatures2
          res = temporal_alignment('find_obj')
        File "Z:\1PhD\UPB_RO\CV_Video_Based_Detection\1\find_obj.py", line 334, in temporal_alignment
          vis = spatial_alignment(win, input_frame, ref_frame, kp_pairs, status, H)
        File "Z:\1PhD\UPB_RO\CV_Video_Based_Detection\1\find_obj.py", line 161, in spatial_alignment
          vis[:hQ, :wQ] = input_frame
      ValueError: operands could not be broadcast together with shapes (576,720) (576,720,3)
    """

    """
    Note that the formal parameters of the caller corresponding to the actual
        parameters input_frame and ref_frame are not changed by the following
        assignments:
    """
    input_frame = common.ConvertImgToGrayscale(input_frame)
    ref_frame = common.ConvertImgToGrayscale(ref_frame)

    display_warped_image = False

    if config.SPATIAL_ALIGNMENT_ALGO in ["LK", "TEMPORAL_ALIGNMENT_HOMOGRAPHY"]:
        if config.SPATIAL_ALIGNMENT_ALGO == "LK":
            # Computing homography with Lucas-Kanade's algorithm:
            h1, status1 = LK.lucas_kanade_homography(input_frame)
            if h1 is not None:
                # We take the H from the LK algorithm
                h = h1
                status = status1

        if config.USE_GUI or config.SAVE_FRAMES:
            # Applying overlay of warped ref_frame with homography H
            h, w = ref_frame.shape[:2]
            overlay = cv2.warpPerspective(src=ref_frame, M=h, dsize=(w, h))

            vis = np.zeros((max(h_q, h_r), w_q + w_r), np.uint8)
            vis[:h_q, :w_q] = input_frame

            if config.DISPLAY_RIGHT_IMAGE_WITH_DIFF:
                # ref_frame is the warped ref_frame, when applying H to it
                ref_frame = overlay

                # TODO: using img_diff = None seems to crash at absdiff().
                #  This is just a crappy assignment, to be of the same type :))
                img_diff = ref_frame

                # Compute difference between frames input_frame and ref_frame:
                cv2.absdiff(src1=input_frame, src2=ref_frame, dst=img_diff)
                vis[:h_r, w_q: w_q + w_r] = img_diff
            else:
                if display_warped_image:
                    ref_frame = overlay
                vis[:h_r, w_q : w_q + w_r] = ref_frame

    elif config.SPATIAL_ALIGNMENT_ALGO == "ECC":
        ECC.template_image = input_frame
        ECC.target_image = ref_frame
        ECC.number_of_iterations = 25
        ECC.termination_eps = 0.001
        ECC.motion = "homography"
        ECC.warp_to_file = "warp.ecc"
        ECC.warp_init = "init.txt"
        ECC.image_to_file = "warped.pgm"

        # Note that ECC.template_image is a numpy.ndarray
        ECC.target_image = Misc.convert_np_to_ipl_image(ECC.target_image)
        ECC.template_image = Misc.convert_np_to_ipl_image(ECC.template_image)

        if config.USE_GUI or config.SAVE_FRAMES:
            vis = np.zeros((max(h_q, h_r), w_q + w_r), np.uint8)
            vis[:h_q, :w_q] = input_frame

            vis[:h_r, w_q: w_q + w_r] = ref_frame
            if config.USE_GUI:
                cv2.imshow(win, vis)
                cv2.waitKey(2000)

        # We require IplImage; the result is basically ECC.warped_image
        ECC.main()

        if config.USE_GUI or config.SAVE_FRAMES:

            # TODO: using img_diff = None seems to crash at absdiff().
            #  This is just a crappy assignment, to be of the same type :))
            img_diff = ref_frame

            ref_frame_new = ECC.warped_image
            print("ref_frame_new (before) = %s" % str(ref_frame_new))

            """
            # We now need to convert ref_frame_new to numpy:
              From https://stackoverflow.com/questions/13104161/fast-conversion-of-iplimage-to-numpy-array:

            "works fantastically, and more important: quickly!
            This is by far the fastest way I've found to grab a frame and make
                it an ndarray for numpy processing." - see code below:
            """
            ref_frame_new = np.asarray(ref_frame_new[:, :])

            print("input_frame = %s" % str(input_frame))
            print("ref_frame = %s" % str(ref_frame))
            print("ref_frame_new = %s" % str(ref_frame_new))

            """
            Because both input_frame and ref_frame are
            OLD:
                input_frame = 
                  <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>
                img_diff = 
                  <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>
                **
                input_frame = 
                  <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>
                ref_frame = [[47 46 47 ..., 59 49 39]
                 [47 47 46 ..., 61 49 39]
                 [46 47 46 ..., 61 49 38]
                 ...,
                 [85 85 86 ..., 79 79 79]
                 [86 86 88 ..., 82 82 82]
                 [88 88 88 ..., 84 83 84]]
            **
            input_frame = 
              <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>
            ref_frame = 
              <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>

            We have the following error:
                "TypeError: src1 is not a numpy array, neither a scalar"
            """
            if config.DISPLAY_RIGHT_IMAGE_WITH_DIFF:
                # Compute difference between frames input_frame and ref_frame:
                cv2.absdiff(src1=input_frame, src2=ref_frame_new, dst=img_diff)
            else:
                img_diff = ref_frame_new

            """
            We display in the right pane the diff between the warped image of
                the target image and the template?
            """
            vis[:h_r, w_q : w_q + w_r] = img_diff

    """
    TODO: return reduced_feature_set - feature_set keypoints of input frame
        that overlap with ref_frame.

    # TODO: in many cases the corners after applying H are just 1 point or a
      line or a very narrow quadrilater - also the feature points
    # We now draw the feature points of the INPUT frame on which we apply H
    ft = np.float32([kpp[1].pt for kpp in kp_pairs]) #!!!!TODO: should be
      kpp[0].pt, NOT the features of the reference frame
    print "ft = %s" % str(ft)
    ftT = cv2.perspectiveTransform(ft.reshape(1, -1, 2), H)
    print "ftT = %s" % str(ftT)
    ft = np.int32(ftT.reshape(-1, 2) + (wQ, 0))
    print "ft (after) = %s" % str(ft)
    for e in ft:
        if keypoint (e[0], e[1])  inside the homography of the rectangle of the
        frame then the keypoint is in reduced_feature_set
    """
    return vis, input_frame, ref_frame, status


# END SPATIAL ALIGNMENT


def cluster_unmatched_keypoints(z):
    # Start time profiling for the inner loop
    t1 = float(cv2.getTickCount())

    z = np.array(z)
    # convert to np.float32
    z = np.float32(z)

    n = len(z)  # the number of points
    common.DebugPrint("N = %d" % n)

    # TODO: should we cluster also the non-matched features of the reference
    #  video (B) ?
    # TODO:_IMPORTANT: now Hierarchical Clustering returns the clusters -
    #  you can color them differently
    z = Clustering.hierarchical_clustering(z, n)

    t2 = float(cv2.getTickCount())
    my_time = (t2 - t1) / cv2.getTickFrequency()

    common.DebugPrint(
        "cluster_unmatched_keypoints(): HierarchicalClustering() "
        "took %.5f [sec]" % my_time)
    return z

# END CLUSTERING


# input_frame, ref_frame are required basically to get again the frame
# dimensions
def annotate_vis(win, vis, input_frame, ref_frame, status_local):
    global vis_orig
    """
    global vis, vis0, winGlobal, p1, p2, statusGlobal
    winGlobal = win
    """

    vis_orig = vis.copy()
    vis = cv2.cvtColor(vis, cv2.COLOR_GRAY2BGR)

    h_q, w_q = input_frame.shape[:2]
    h_r, w_r = ref_frame.shape[:2]

    blue = (255, 0, 0)
    pink = (255, 128, 255)
    white = (255, 255, 255)

    kp_color = (51, 103, 236)
    red = (0, 0, 255)
    green = (0, 255, 0)
    green_gray = (51, 173, 136)

    if config.USE_GUI or config.SAVE_FRAMES:
        # DRAW THE KEYPOINTS

        # Alex: we represent also ALL THE features of the 2 images:
        for e in kp1[counter_q]:
            cv2.circle(vis, (int(e.pt[0]), int(e.pt[1])), 10, blue, -1)

        # We draw for image2, hence + wQ translation on horizontal
        for e in kp2[counter_r]:
            cv2.circle(vis, (int(e.pt[0] + w_q), int(e.pt[1])), 10, pink, -1)

        """
        # Alex: we represent also all the matched features of the 2 images:
        for e in ftImg1:
            cv2.circle(vis, (e[0], e[1]), 4, blue, -1)
        # We draw for image2, hence + wQ translation on horizontal
        for e in ftImg2:
            #print "e =", e
            cv2.circle(vis, (int(e[0]) + int(wQ), int(e[1])), 5, pink, -1)
            #cv2.circle(vis, (e[0], e[1]), 2, pink, -1)
        """

        """
        Note: nonp1 is updated by Clustering.HierarchicalClustering() - it
            represents the non-matched features that form a dense
            cluster of more than THRESHOLD_NUM_NONMATCHED_ELEMENTS_IN_CLUSTER
            elements.

        We draw the NON-matched features for frame of video A:
        """
        for e in nonp1:
            cv2.circle(vis, (int(e[0]), int(e[1])), 7, green_gray, -1)

        # We draw the NON-matched features for frame of video B:
        for e in nonp2:
            cv2.circle(vis, (int(e[0] + w_q), int(e[1])), 7, green_gray, -1)

        # END DRAW THE KEYPOINTS

    if config.USE_GUI or config.SAVE_FRAMES:
        """
        We draw the homography transformation by looking at the identified
            features - H is basically a rotation and zoom (transformation in
            homogenous?? coordinates).
        """
        if H is not None:
            corners = [[0, 0], [w_q, 0], [w_q, h_q], [0, h_q]]
            print("corners = %s" % str(corners))

            corners = np.float32(corners)

            # See http://stackoverflow.com/questions/6627647/reshaping-a-numpy-array-in-python for description of numpy.reshape()
            try:
                cornersT = cv2.perspectiveTransform(
                    src=corners.reshape(1, -1, 2), m=H)
                print("cornersT = %s" % str(cornersT))
                corners = np.int32(cornersT.reshape(-1, 2) + (w_q, 0))
                print("corners = %s" % str(corners))
            except:
                common.DebugPrintErrorTrace()
                common.DebugPrint("Exception at perspectiveTransform(): H=%s" %
                                  str(H))
                corners = np.int32(corners.reshape(-1, 2) + (w_q, 0))

            """
            print "cornersT = %s" % str(cornersT)
            corners = np.int32(cornersT.reshape(-1, 2) + (wQ, 0))
            print "corners = %s" % str(corners)
            """
            cv2.polylines(img=vis, pts=[corners], isClosed=True,
                          color=(255, 255, 255))

    if config.USE_GUI:
        if status_local is None:
            status_local = np.ones(len(kp_pairs), np.bool_)

        if not kp_pairs:
            p1_local = []
            p2_local = []
        else:
            p1_local = np.int32([kpp[0].pt for kpp in kp_pairs])
            p2_local = np.int32([kpp[1].pt for kpp in kp_pairs]) + (w_q, 0)

        common.DebugPrint("len(p1_local) = %d" % len(p1_local))
        common.DebugPrint("len(kp_pairs) = %d" % len(kp_pairs))

        for (x1, y1), (x2, y2), inlier in zip(p1_local, p2_local, status_local):
            if inlier:
                col = green
                cv2.circle(vis, (x1, y1), 5, col, -1)
                cv2.circle(vis, (x2, y2), 8, col, -1)
            else:
                col = red
                r = 2
                thickness = 4
                # We draw some sort of squares for the
                cv2.line(vis, (x1 - r, y1 - r), (x1 + r, y1 + r), col,
                         thickness)
                cv2.line(vis, (x1 - r, y1 + r), (x1 + r, y1 - r), col,
                         thickness)
                cv2.line(vis, (x2 - r, y2 - r), (x2 + r, y2 + r), col,
                         thickness)
                cv2.line(vis, (x2 - r, y2 + r), (x2 + r, y2 - r), col,
                         thickness)

        """
          They use it when clicking on mouse to make the corresponding lines
            disappear between the 2 images.
        """
        vis0 = vis_orig.copy()

        # This is where we draw the green lines between the 2 images
        for (x1, y1), (x2, y2), inlier in zip(p1_local, p2_local, status_local):
            if inlier:
                cv2.line(vis, (x1, y1), (x2, y2), green, 2)

        cv2.imshow(win, vis)

        """
        # Used if we make onmouse() a global function
        statusGlobal = statusLocal
        """

        """
        IMPORTANT: a limitation of Python 2.x is that inner functions
            do a symboltable lookup of non-local variables in the global
            scope not the immediately outter scope of the function - in
            this case the one of annotate_vis().
          This is why we differentiate variables with the same name:
            statusLocal and statusGlobal
            p1_local and p1 .
        """

        def onmouse(event, x, y, flags, param):
            crt_vis = vis
            if flags & cv2.EVENT_FLAG_LBUTTON:
                crt_vis = vis0.copy()
                r = 3

                m = (common_cv.anorm(p1_local - (x, y)) < r) | \
                    (common_cv.anorm(p2_local - (x, y)) < r)
                idxs = np.where(m)[0]
                kp1s, kp2s = [], []

                assert len(p1_local) == len(p2_local)

                for i in idxs:
                    (x1, y1), (x2, y2) = p1_local[i], p2_local[i]

                    try:
                        col = (red, green)[status_local[i]]
                    except:
                        common.DebugPrintErrorTrace()
                        common.DebugPrint("onmouse() exception: i = %d, "
                                          "len(statusLocal) = %d" % (
                                              i, len(status_local)))
                        col = red

                    cv2.line(crt_vis, (x1, y1), (x2, y2), col)

                    try:
                        kp1, kp2 = kp_pairs[i]
                        kp1s.append(kp1)
                        kp2s.append(kp2)
                    except:
                        common.DebugPrintErrorTrace()
                        common.DebugPrint("onmouse() exception2: i = %d, "
                                          "len(kp_pairs)=%d, len(idxs)=%d, idxs=%s" %
                                          (i, len(kp_pairs), len(idxs),
                                           str(idxs)))

                crt_vis = cv2.drawKeypoints(crt_vis, kp1s, flags=4,
                                            color=kp_color)
                crt_vis[:, w_q:] = cv2.drawKeypoints(crt_vis[:, w_q:], kp2s,
                                                     flags=4, color=kp_color)

            cv2.imshow(win, crt_vis)

        cv2.setMouseCallback(win, onmouse)

    return vis


def rest(win):
    global nonp1
    global status

    # Here we perform spatial alignment and clustering and display the results.
    t1 = float(cv2.getTickCount())

    # TODO: filter out p1 outside the alignment of the pair of frames
    (myVis, inputFrame, refFrame, status2) = spatial_alignment(win, img_q,
                                                               img_r,
                                                               kp_pairs, status,
                                                               H)

    t2 = float(cv2.getTickCount())
    my_time = (t2 - t1) / cv2.getTickFrequency()
    common.DebugPrint(
        "feature_match_and_homography(): spatial_alignment() took %.6f [sec]" %
        my_time)

    # TODO: filter out nonp1 outside the alignment of the pair of frames -
    #  should do it on p1 inside spatial_alignment()
    # We cluster the non-matched keypoints
    nonp1 = cluster_unmatched_keypoints(nonp1)

    my_vis2 = annotate_vis(win, myVis, inputFrame, refFrame, status)

    # TODO: remove global definition of vis
    global vis
    vis = my_vis2

    if config.USE_GUI:
        ch = cv2.waitKey()
        print("ch = %s" % str(ch))
        if ch == 27:  # Escape key
            quit()

        cv2.destroyAllWindows()


# TEMPORAL ALIGNMENT

def compute_features_1(a_img_1, a_counter_q):
    global kp1, desc1
    global counter_q
    global img_q

    img_q = a_img_1

    counter_q = a_counter_q

    common.DebugPrint("compute_features_1(): counter_q = %d" % counter_q)

    if kp1[counter_q] is None:
        t1 = float(cv2.getTickCount())

        kp1[counter_q], desc1[counter_q] = detector.detectAndCompute(a_img_1,
                                                                     None)

        t2 = float(cv2.getTickCount())
        my_time = (t2 - t1) / cv2.getTickFrequency()
        common.DebugPrint(
            "compute_features_1(): detector.detectAndCompute() took %.6f [sec]" %
            my_time)

    common.DebugPrint("a_img_1 - %d features" % len(kp1[counter_q]))


def compute_features_and_match_2(a_img_r, a_counter_r):
    global kp2, desc2
    global counter_r
    global img_r

    img_r = a_img_r

    counter_r = a_counter_r

    if counter_r is None:
        counter_r = 0

    common.DebugPrint(
        "compute_features_and_match_2(): counter_r = %d (counter_q = %d)" %
        (counter_r, counter_q))

    # If we did NOT memoize kp2[counter_r], desc2[counter_r] we compute
    # them once
    if kp2[counter_r] is None:
        """
        From http://docs.opencv.org/trunk/modules/nonfree/doc/feature_detection.html
            "Python API provides three functions. 
             First one finds keypoints only.
             Second function computes the descriptors based on the keypoints we
                provide.
             Third function detects the keypoints and computes their descriptors
             If you want both keypoints and descriptors, directly use third
                function as kp, des = cv2.SIFT.detectAndCompute(image, None)"
        """
        t1 = float(cv2.getTickCount())

        kp2[counter_r], desc2[counter_r] = detector.detectAndCompute(a_img_r,
                                                                     None)

        t2 = float(cv2.getTickCount())
        my_time = (t2 - t1) / cv2.getTickFrequency()
        common.DebugPrint("compute_features_and_match_2(): "
                          "detector.detectAndCompute() took %.6f [sec]" %
                          my_time)

    common.DebugPrint("a_img_r - %d features" % len(kp2[counter_r]))

    res = feature_match_and_homography()

    """
    The result is related to the homography transformation returned by
            feature_match_and_homography(): status sum and len
    """
    return res


myTemporalAlignmentCost = 0
"""
filter_keypoints_matches() has a time complexity of O(len(matches))
Note: 0.99 is a very permissive thershold and some matches are ~bogus
"""


def filter_keypoints_matches(kp1, kp2, matches, ratio=0.95):
    global nonp1, nonp2
    global myTemporalAlignmentCost

    myTemporalAlignmentCost = 0

    mkp1, mkp2 = [], []
    nonp1, nonp2 = [], []

    index = 0
    for m in matches:
        len_m = len(m)
        if len_m == 2:
            if m[0].distance < m[1].distance * ratio:
                """
                Inspired from
                    http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf#page=20:
                  "A more effective measure is obtained by comparing the
                    distance of the closest neighbor to that of the
                    second-closest neighbor."
                  Note:
                    m[0] = closest neighbor match
                    m[1] = 2nd-closest neighbor match
                """

                # These keypoints are considered matched -
                # we choose the 1st closest match

                mkp1.append(kp1[m[0].queryIdx])
                mkp2.append(kp2[m[0].trainIdx])

                myTemporalAlignmentCost += m[0].distance
            else:
                # These keypoints are considered NOT matched
                nonp1.append(kp1[m[0].queryIdx].pt)
                # TODO: I think that nonp2.append below is NOT relevant
        else:
            assert len_m <= 2

            # These keypoints are considered NOT matched
            if len_m == 1:
                nonp1.append(kp1[m[0].queryIdx].pt)
            else:
                nonp1.append(kp1[index].pt)
        index += 1

    """
    nonp1 is a list of pairs of x, y coordinates for the features/keypoints
        that have not been matched from frame of video A.
    """
    common.DebugPrint("len(nonp1) = %d" % len(nonp1))
    common.DebugPrint("len(nonp2) = %d" % len(nonp2))

    p1 = np.float32([kp.pt for kp in mkp1])
    p2 = np.float32([kp.pt for kp in mkp2])
    kp_pairs = zip(mkp1, mkp2)

    return p1, p2, kp_pairs


"""
Part of temporal alignment.

TODO: We currently use a ~Lazy evaluation might not be best for cache (I-cache and D-cache).
    I guess the best is to have first preprocessed completely video 1,
      then preprocessed video2 then do a match between the 2 videos.
"""


def feature_match_and_homography():
    # TODO: memoize (but it's gonna be huge) the pair results of matching -
    #  should reduce by 2 the runtime
    global p1, p2, kp_pairs, status, H

    print("Entered feature_match_and_homography()...")

    # raw_matches still has the same len as desc1[counter_q] AND trainDescriptors

    t1 = float(cv2.getTickCount())

    """
    opencv2refman.pdf, Section 7.4., page 429:
      knnMatch
        "Finds the k best matches for each descriptor from a query set."
        "k - Count of best matches found per each query descriptor or less if a
            query descriptor has less than k possible matches in total."
    """
    raw_matches = matcher.knnMatch(desc1[counter_q],
                                   trainDescriptors=desc2[counter_r], k=2)

    p1, p2, kp_pairs = filter_keypoints_matches(kp1[counter_q], kp2[counter_r],
                                                raw_matches)

    t2 = float(cv2.getTickCount())
    my_time = (t2 - t1) / cv2.getTickFrequency()
    common.DebugPrint(
        "feature_match_and_homography(): knnMatch() and "
        "filter_keypoints_matches() took %.6f [sec]" % my_time)

    """
    # We cluster & display the non-matched keypoints
    nonp1 = cluster_unmatched_keypoints(nonp1)
    """

    """
    raw_matches is a list of corresponding?? pairs of DMatch for the keypoints
        obtained with knnMatch() .

    p1, p2 are only the matched keypoints from the 2 frames

    kp_pairs is pair of corresponding keypoints, obtained with the list zip
    operation:
        cardinality ~=???? the cardinality of kp1[counter_q] and kp2[counter_r]
    """

    if len(p1) >= 4:
        # p1 and p2 are the matched keypoints

        """
        From Section 6.1, page 391, opencv2refman.pdf:
            (also http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography#findhomography):
              "Finds a perspective transformation between two planes."
              "However, if not all of the point pairs (srcPoints
              i ,:math:dstPoints_i ) fit the rigid perspective transformation 
              (that is, there are some outliers), this initial estimate will be
              poor."

          "In this case, you can use one of the two robust methods.
          Both methods, RANSAC and LMeDS , try many different random subsets
            of the corresponding point pairs (of four pairs each), estimate
            the homography matrix using this subset and a simple least-square
            algorithm, and then compute the quality/goodness of the computed
            homography (which is the number of inliers for RANSAC or the median
            re-projection error for LMeDs). The best subset is then used to
            produce the initial estimate of the homography matrix and the
            mask of inliers/outliers."
        """
        t1 = float(cv2.getTickCount())

        H, status = cv2.findHomography(srcPoints=p1, dstPoints=p2,
                                       method=cv2.RANSAC,
                                       ransacReprojThreshold=5.0)

        t2 = float(cv2.getTickCount())
        my_time = (t2 - t1) / cv2.getTickFrequency()
        common.DebugPrint(
            "feature_match_and_homography(): cv2.findHomography() "
            "took %.6f [sec]" % my_time)

        if status is None:
            status = []

        if H == None:
            print("!!!!!!!!found H None - len(p1) = %d" % len(p1))
            H = []
            return -1, len(p1)

        print("%d / %d  inliers/matched (len(p1) = %d)" %
              (np.sum(status), len(status), len(p1)))

        # Note: H is the homography matrix, a 3x3 matrix.
        common.DebugPrint(
            "H, the homography matrix, from cv2.findHomography = %s" % str(H))

        common.DebugPrint("     len(H) = %d" % len(H))

        # Note that len(status) == len(p1)
        res = (np.sum(status), len(status))
    else:
        H, status = None, None
        common.DebugPrint(
            "%d matches found, not enough for homography estimation" % len(p1))

        res = (-1, len(p1))

    # The result is related to the homography transformation: status sum and len
    return res


"""
Avoid using so many globals.

Should we have modules:
    temporal_alignment
    spatial_alignment
    Clustering
  ?


!!!!TODO !!!!!!!! Do better interface for the processing pipeline:
    e.g., TODO
      - implement return (frame, ref_frame, feature_set, ref_feature_set)
         (frame, ref_frame, feature_set, ref_feature_set)
           - frame este un frame din filmul curent
           - ref_frame este frame-ul cel mai apropiat din filmul referinta
           - feature_set este multimea descriptorilor SIFT/ORB/whatever extrasi din frame
           - ref_feature_set este multimea descriptorilor SIFT/ORB/whatever extrasi din ref_frame

      - pasul de spatial aligment urmareste sa identificam sectiunile
        din frame care nu se regasesc in ref_frame, si sa eliminam din feature_set
        acele features care cad in afara zonei de suprapunere intre frame si
        ref_frame.

         Pasul de aliniere spatiala livreaza catre clustering un tuplu (frame,
          reduced_feature_set), unde:
          - frame este un cadru din filmul curent (nici o modificare fata de ce am
                      primit de la alinierea temporala)
          - reduced_feature_set este submultimea lui feature_set care indeplineste
              conditia ca feature-ul se afla intr-o zona de suprapunere a lui
              frame cu ref_frame

      - pasul Clustering livreaza urmatoarele: (frame, cluster_areas), unde:
        - frame iarasi e nemodificat
        - cluster_areas reprezinta definitiile spatiale ale clusterelor identificate

Currently the rest() of the pipepline is called by temporal_alignment, when
    found an optimal candidate.

Temporal alignment:
    - for each reference frame:
        - we also detect features (and compute descriptors) for reference frame
        - feature_match_and_homography()
            - knnMatch()
            - filter_keypoints_matches()
            - homography
"""


def temporal_alignment(counter_q, frame_q, capture_r, num_frames_r,
    num_features_matched, f_output):
    global p1, p2, kp_pairs, status, H, nonp1

    if config.USE_EXHAUSTIVE_SEARCH:
        max_features_matched = -2000000000
        pos_max_features_matched = -1

        while True:
            if config.OCV_OLD_PY_BINDINGS:
                frameR = capture_r.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)
            else:
                frameR = capture_r.get(cv2.CAP_PROP_POS_FRAMES)
            common.DebugPrint("Alex: frameR = %d" % frameR)

            counter_r = int(frameR)

            ret2, img_r = capture_r.read()
            if (ret2 == False) or (counter_r > num_frames_r):
                break

            if config.SAVE_FRAMES:
                file_name = config.IMAGES_FOLDER + "/imgR_%05d.png" % counter_r
                if not os.path.exists(file_name):
                    cv2.imwrite(file_name, img_r)

            res = compute_features_and_match_2(img_r, counter_r)

            cost_used = 1
            if cost_used == 0:
                pass
            else:
                # myTemporalAlignmentCost is the sum of distances of best-pairs
                # (closest neighbors) of matched features
                res = (res[0], -myTemporalAlignmentCost)

            num_features_matched[counter_q][counter_r] = res[1]

            compute_best_fast = False

            if max_features_matched < res[1]:
                max_features_matched = res[1]
                pos_max_features_matched = counter_r

                if config.SAVE_FRAMES:
                    if not compute_best_fast:
                        # TODO: don't do it even for best candidates -
                        #  only for the best ONE - this implies redoing probably
                        #  some computation from temporal_alignment() for the
                        #  best frame pair
                        # We call rest() in order to compute vis and visOrig.

                        rest("Image Match")

                        vis_best = vis.copy()
                        vis_orig_best = vis_orig.copy()
                    else:
                        p1_best = p1
                        p2_best = p2
                        kp_pairs_best = kp_pairs
                        status_best = status
                        h_best = H
                        nonp1_best = nonp1

            common.DebugPrint("Alex: counter_r = %d" % counter_r)

            counter_r += config.counterRStep

            """
            If we try to seek to a frame out-of-bounds frame it gets to
                the last one.
            """
            if config.OCV_OLD_PY_BINDINGS:
                capture_r.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, counter_r)
            else:
                capture_r.set(cv2.CAP_PROP_POS_FRAMES, counter_r)

            common.DebugPrint("Alex: Time = %s" %
                              common.GetCurrentDateTimeStringWithMilliseconds())

        my_text = "  Frame #%d of video A: frame #%d of video B, " \
                  "with %d features matched (time = %s)" % \
                  (counter_q, pos_max_features_matched, max_features_matched,
                   common.GetCurrentDateTimeStringWithMilliseconds())

        print(my_text, file=f_output)
        f_output.flush()

        counter_r_best = pos_max_features_matched
    else:
        # We empty the memoization cache before SimAnneal.main()
        SimAnneal.Acache = {}

        res = SimAnneal.main()
        res2 = (res[0], -res[1])

        common.DebugPrint(
            "Best solution for frame counter_q=%d is %s. Time = %s" %
            (counter_q, str(res2),
             common.GetCurrentDateTimeStringWithMilliseconds()))

        # TODO: check if OK:
        counter_r_best = res[0]

    if compute_best_fast:
        # TODO: check that these assignments really refer the objects defined
        #  above (when updating) and that there are no escapes of the
        #  values/objects that result in side-effects updating the respective
        #  objects and messing everything up - better said we look if
        #  redefinitons of the rhs are done inside (some of their subelements)
        #  or totally (reassign a completely NEW object).

        p1 = p1_best
        p2 = p2_best
        kp_pairs = kp_pairs_best
        status = status_best
        H = h_best
        nonp1 = nonp1_best

        rest("Image Match")

        vis_best = vis.copy()
        vis_orig_best = vis_orig.copy()

    if config.SAVE_FRAMES:
        """
        We display frames imgQ and img_r with features (from temporal) and
            clusters on them.
        """
        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER +
                    "/img_proc_%05d_%05d.png" %
                    (counter_q, counter_r_best), vis_best)

        # We display also the orig frames imgQ and img_r
        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER +
                    "/img_proc_%05d_%05d_orig.png" %
                    (counter_q, counter_r_best), vis_orig_best)

    """
    We return the len of the status returned by cv2.findHomography() - this is
        THE indicator of the match of a pair of frames.
    NOTE: we don't use the result :)))
    !!!!TODO: think to take it out
    """
    return res[1]  # res


# END TEMPORAL ALIGNMENT

# TODO: This function doesn't work, finish or delete.
"""
def do_the_rest_after_temporal_alignment():
    global p1, p2, kp_pairs, status, h, nonp1
    # TODO: the *Best vars below need to be communicated either via return or
    #  made global

    p1 = p1_best
    p2 = p2_best
    kp_pairs = kp_pairs_best
    status = status_best
    h = h_best
    nonp1 = nonp1_best

    rest("Image Match")
"""


def process_input_frames(capture_q, capture_r, f_output):
    # Allocate num_features_matched
    num_features_matched = [None] * num_frames_q
    for i in range(num_frames_q):
        num_features_matched[i] = [-2000000000] * num_frames_r

    while True:
        if config.OCV_OLD_PY_BINDINGS:
            frame_q = capture_q.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)
        else:
            frame_q = capture_q.get(cv2.CAP_PROP_POS_FRAMES)
        common.DebugPrint("Alex: frame_q = %d" % frame_q)

        counter_q = int(frame_q)
        common.DebugPrint("Alex: counter_q = %d" % counter_q)

        ret1, imgQ = capture_q.read()

        if False and config.SAVE_FRAMES:
            file_name = config.IMAGES_FOLDER + "/imgQ_%05d.png" % counter_q
            if not os.path.exists(file_name):
                cv2.imwrite(file_name, imgQ)

        if (ret1 == False) or (counter_q > num_frames_q):
            break

        # TODO: counter_q already visible in module MatchFrames
        compute_features_1(imgQ, counter_q)

        # We set the video stream capture_r at the beginning
        if config.OCV_OLD_PY_BINDINGS:
            capture_r.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, 0)
        else:
            capture_r.set(cv2.CAP_PROP_POS_FRAMES, 0)

        # Start time profiling for the inner loop
        t1 = float(cv2.getTickCount())

        # TODO: counter_q already visible in module MatchFrames
        temporal_alignment(counter_q, frame_q, capture_r,
                           num_frames_r, num_features_matched, f_output)

        # Measuring how much it takes the inner loop
        t2 = float(cv2.getTickCount())
        my_time = (t2 - t1) / cv2.getTickFrequency()
        common.DebugPrint(
            "Avg time it takes to complete a match (and to perform "
            "INITIAL Feat-Extract) = %.6f [sec]" %
            (my_time / (num_frames_r / config.counterRStep)))

        counter_q += config.counterQStep
        # If we try to seek to a frame out-of-bounds frame it gets to the
        # last one
        if config.OCV_OLD_PY_BINDINGS:
            capture_q.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, counter_q)
        else:
            capture_q.set(cv2.CAP_PROP_POS_FRAMES, counter_q)

    common.DebugPrint("num_features_matched = %s" % str(num_features_matched))


if __name__ == '__main__':
    pre_main(n_frames_q=1000, n_frames_r=1000)
    compute_features_and_match_2(None, None)
